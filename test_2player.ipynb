{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-gpu (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for tensorflow-gpu\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --force-reinstall tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from ddqn_keras import DDQNAgent\n",
    "# from utils import plotLearning\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience Replay\n",
    "class ReplayBuffer():\n",
    "    def __init__(self,memory_size=100000, input_shape=(9,)):\n",
    "        self.memory_size = int(memory_size/1.5)\n",
    "        self.input_shape = input_shape\n",
    "        # memory counter -> tracks first unsaved memory and use it to insert new memory into buffer\n",
    "        # When replay buffer becomes full, it rewrites earliest memoreis\n",
    "        self.memory_counter = 0\n",
    "        # TODO: Change *input_shape to directly the elements\n",
    "#         print(input_shape)\n",
    "#         print(type(self.memory_size/2))\n",
    "        self.state_memory = np.zeros((self.memory_size, *input_shape), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.memory_size, *input_shape), dtype=np.float32)\n",
    "        # TODO: Figure out why np.int32\n",
    "\n",
    "        # reward memory = \n",
    "        # terminal memory is the memory where it ends? done flags?\n",
    "        self.action_memory = np.zeros(self.memory_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.memory_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.memory_size, dtype=np.float32)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = 1-int(done)\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        # Have we filled up the agents memory or not. If we have we need to sample a bunch of zeroes\n",
    "        \n",
    "        # If we haven't filled up the agent's memory we shouldn't sample it\n",
    "        # why? Not sure\n",
    "        # If we filled it up -> we sample full memory \n",
    "        # If we haven't -> we sample till the memory size, because the rest doesn't give it any significant information\n",
    "\n",
    "        # TODO: Remove this option and test it out\n",
    "        current_memory = min(self.memory_counter, self.memory_size)\n",
    "        # TODO: Constant batch size\n",
    "        batch = np.random.choice(current_memory, batch_size, replace=False)\n",
    "\n",
    "\n",
    "        # TODO: Answer can you have it as one single list that can be sampled? We need to figure that out. \n",
    "        # In that case we can remove current_memory too out of the requirement\n",
    "        # Just pick the entire batch baby. \n",
    "\n",
    "        # THEN WE JUST SAMPLE THE MEMORY\n",
    "        state = self.state_memory[batch]\n",
    "        state_ = self.new_state_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        # TODO: Instead of the above todo, we can select random choice the s,a,r,s',t from the list directly of size, batch size.\n",
    "        # No np bullshit\n",
    "        # Simpler? YES! DEFINITELY! RESOLVED\n",
    "\n",
    "\n",
    "        return state, actions, rewards, state_, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "# from rl.deep_q_network import DeepQNetwork\n",
    "# from game import Game\n",
    "# from gui import GUI\n",
    "import time\n",
    "import threading\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from rl.replay_buffer import ReplayBuffer\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "#----------------------Game {------------------------------------------------\n",
    "#----------------------------------------------------------------------------\n",
    "# GAME\n",
    "\n",
    "import copy\n",
    "\n",
    "class Env:\n",
    "    def __init__(self):\n",
    "        self.board = np.array([-1.0] * 9)\n",
    "        self.allowedAction = np.where([-1.0] * 9)\n",
    "        self.winning_combos = (\n",
    "        [6, 7, 8], [3, 4, 5], [0, 1, 2], [0, 3, 6], [1, 4, 7], [2, 5, 8],\n",
    "        [0, 4, 8], [2, 4, 6],)\n",
    "        self.corners = [0,2,6,8]\n",
    "        self.sides = [1,3,5,7]\n",
    "        self.middle = 4\n",
    "\n",
    "    def get_marker(self):\n",
    "        return (1.0,0.0)\n",
    "    \n",
    "    def set_board(self, board):\n",
    "        self.board = board\n",
    "        self.allowedAction = list(np.where(self.board==-1)[0])\n",
    "        return board\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.array([-1.0] * 9)\n",
    "        return self.board\n",
    "    \n",
    "    def get_board(self):\n",
    "        return self.board.reshape(3,3)\n",
    "\n",
    "    def step(self, action, mark):\n",
    "        over = False\n",
    "        reward = 0\n",
    "\n",
    "        self.make_move(self.board, action, mark)\n",
    "        \n",
    "        if(self.is_winner(self.board,mark)):\n",
    "            reward = 100\n",
    "            over = True\n",
    "\n",
    "        # drawing\n",
    "        elif self.is_board_full():\n",
    "            reward = 10\n",
    "            over = True\n",
    "        \n",
    "#         print(self.board)\n",
    "\n",
    "        self.allowedAction = list(np.where(self.board==-1)[0])\n",
    "\n",
    "        return self.board,self.allowedAction, reward, over\n",
    "\n",
    "    def is_winner(self, board, mark):\n",
    "        for combo in self.winning_combos:\n",
    "            if (board[combo[0]] == board[combo[1]] == board[combo[2]] == mark):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_winning_combo(self, board):\n",
    "        for combo in self.winning_combos:\n",
    "            if (board[combo[0]] == board[combo[1]] == board[combo[2]]):\n",
    "                return [combo[0], combo[1], combo[2]]\n",
    "        return [None, None, None]\n",
    "\n",
    "    def is_space_free(self, board, index):\n",
    "        \"checks for free space of the board\"\n",
    "        return board[index] == -1.0\n",
    "\n",
    "    def is_board_full(self):\n",
    "        \"checks if the board is full\"\n",
    "        for i in range(1,9):\n",
    "            if self.is_space_free(self.board, i):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def make_move(self,board,index, mark):\n",
    "        board[index] =  mark\n",
    "\n",
    "    def choose_random_move(self, move_list):\n",
    "        possible_winning_moves = []\n",
    "        for index in move_list:\n",
    "            if self.is_space_free(self.board, index):\n",
    "                possible_winning_moves.append(index)\n",
    "        if len(possible_winning_moves) != 0:\n",
    "            return random.choice(possible_winning_moves)\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allowed = [3]\n",
    "a = np.array([1,1,1,-1,0])\n",
    "np.take(a,allowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self, epsilon=1., min_epsilon=0.1, epsilon_decay=5e-4):\n",
    "        self.epsilon = 1\n",
    "        self.batch_size = 50\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.game = Env()\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(4, input_dim=9, activation='relu'))\n",
    "        self.model.add(Dense(4, activation='relu'))\n",
    "        self.model.add(Dense(9, activation='sigmoid'))\n",
    "        self.model.add(Activation('softmax'))\n",
    "        self.model.compile(loss='mse', optimizer='adam')\n",
    "        self.buffer = ReplayBuffer(input_shape = (9,) , memory_size = 100000)\n",
    "#         self.model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    def take_action(self, allowed_actions, state):\n",
    "#         FOR SELECTING RANDOM\n",
    "        random = np.random.choice([True, False], 1, p=[self.epsilon,1-self.epsilon])[0]\n",
    "#     UPDATE EPSILON VALUE\n",
    "        self.epsilon = max(self.epsilon-self.epsilon_decay, self.min_epsilon)\n",
    "        \n",
    "#         IF RANDOM WHAT DO I DO!!!!\n",
    "        all_actions = np.arange(9)\n",
    "\n",
    "#         IF RANDOM WE GO WITH THIS TWO LINES\n",
    "        allowed_actions_pos = np.take(all_actions, allowed_actions)\n",
    "        action = np.random.choice(allowed_actions_pos)\n",
    "            \n",
    "        if not random:\n",
    "            actions = self.model.predict(np.array([state]))[0]\n",
    "            max_action = 0\n",
    "            for k,v in enumerate(list(actions)):\n",
    "                if k in allowed_actions:\n",
    "                    if v>max_action:\n",
    "                        max_action=v\n",
    "                        action = k\n",
    "        return action\n",
    "    \n",
    "    def play_with_self(self):\n",
    "        self.windraw_counter = 0\n",
    "        self.nwin_list = []\n",
    "        self.ndraw_list = []\n",
    "        for i in range(20000):\n",
    "            self.game.reset()\n",
    "            done = False\n",
    "            if i%5000 == 0:\n",
    "                print(\"rounds: \", i)\n",
    "            step = 0\n",
    "            allowed_actions = np.arange(9)\n",
    "            state = self.game.board\n",
    "            self.n_wins = 0\n",
    "            self.n_draw = 0\n",
    "            \n",
    "            while not done:\n",
    "#                 Starting game\n",
    "                step+=1\n",
    "#                 When player 1 is done, player 2 get's lower reward\n",
    "\n",
    "    \n",
    "#                 Player 1\n",
    "                action = self.take_action(allowed_actions, state)\n",
    "                state_,allowed_actions,r,done = self.game.step(action=action,mark=0)\n",
    "                \n",
    "#                 Player 2\n",
    "                if not done:\n",
    "                    action2 = self.take_action(allowed_actions, state_)\n",
    "                    state2_,allowed_actions,r2,done = self.game.step(action=action2,mark=1)\n",
    "                    self.buffer.store_transition(state_, action2, r2, state2_, done)\n",
    "                if r2 == 100:\n",
    "                    r = -r2\n",
    "\n",
    "#                 Add both to ReplayBuffer\n",
    "                self.buffer.store_transition(state, action, r, state_, done)\n",
    "                \n",
    "                self.learn()\n",
    "                state = state2_\n",
    "                if done and r==100:\n",
    "                    self.n_wins += 1\n",
    "                elif done and r==0:\n",
    "                    self.n_draw +=1\n",
    "            \n",
    "            self.windraw_counter+=1\n",
    "            if self.windraw_counter%10 == 0:\n",
    "                self.nwin_list.append(self.n_wins)\n",
    "                self.ndraw_list.append(self.n_wins)\n",
    "            \n",
    "    \n",
    "    def learn(self):\n",
    "        if self.buffer.memory_counter<self.batch_size:\n",
    "            return\n",
    "        \n",
    "        state, action, rewards, state_, dones = self.buffer.sample_buffer(self.batch_size)\n",
    "\n",
    "        # Finding a q value for a bunch of states and next states\n",
    "        q = self.model.predict(state)\n",
    "        q_next = self.model.predict(state_)\n",
    "\n",
    "        # Q value to be updated -> basically the weights to be updated such that the q value is changed to whatever it is supposed to be. \n",
    "        # Therefore \n",
    "        # TODO: Change the method of doing this. Figure out a way of doing it without np.copy\n",
    "        # maybe np.array(q_eval) will return a new array. \n",
    "        # Test it out on jupyter\n",
    "        q_target = np.copy(q)\n",
    "\n",
    "        # np.arange(self.batch_size, dtype=np.int32) -> handles array indexing\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "#       MaxQ\n",
    "        max_of_q_of_each_sample = np.max(q_next, axis=1)\n",
    "#     Gamma = 0.9\n",
    "        q_target[batch_index, action] = rewards+0.9*max_of_q_of_each_sample*dones\n",
    "\n",
    "#       IS THERE ANY OTHER WAY OF WRITING THIS?\n",
    "#       Neural Network style of updating a q table. Update weights\n",
    "        self.model.train_on_batch(state, q_target)\n",
    "\n",
    "        # TODO: Update self.epsilon\n",
    "\n",
    "        self.epsilon = self.epsilon - self.epsilon_decay if self.epsilon>self.min_epsilon else self.min_epsilon\n",
    "\n",
    "    def play(self):\n",
    "#         if i%1000 == 0:\n",
    "        done = False\n",
    "        step = 0\n",
    "        allowed_actions = np.arange(9)\n",
    "        self.game.reset()\n",
    "        state = self.game.board\n",
    "        \n",
    "        while not done:\n",
    "#                 Starting game\n",
    "            step+=1\n",
    "#                 Player 1\n",
    "            action = self.take_action(allowed_actions, state)\n",
    "            state_,allowed_actions,r,done = self.game.step(action=action,mark=0)\n",
    "            \n",
    "#                 Player 2\n",
    "            print('Game: ')\n",
    "            print(self.game.get_board())\n",
    "            print(\"Enter your value among :\") \n",
    "            print(self.game.allowedAction)\n",
    "            played_right = False\n",
    "            while not played_right:\n",
    "                try:\n",
    "                    action2 = int(input('enter value'))\n",
    "                    played_right = True\n",
    "#                     if action2 not in allowed_actions:\n",
    "#                         print(0/0)\n",
    "                except:\n",
    "                    print(\"Enter the right value in integer from 0 to 8\")\n",
    "            state = state_\n",
    "            \n",
    "            state2_,allowed_actions,r2,done = self.game.step(action=action2,mark=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rounds:  0\n",
      "rounds:  5000\n",
      "rounds:  10000\n",
      "rounds:  15000\n"
     ]
    }
   ],
   "source": [
    "game = Game()\n",
    "game.play_with_self()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game: \n",
      "[[-1. -1. -1.]\n",
      " [-1. -1. -1.]\n",
      " [ 0. -1. -1.]]\n",
      "Enter your value among :\n",
      "[0, 1, 2, 3, 4, 5, 7, 8]\n",
      "enter value4\n",
      "Game: \n",
      "[[-1. -1. -1.]\n",
      " [-1.  1. -1.]\n",
      " [ 0. -1.  0.]]\n",
      "Enter your value among :\n",
      "[0, 1, 2, 3, 5, 7]\n",
      "enter value0\n",
      "Game: \n",
      "[[ 1.  0. -1.]\n",
      " [-1.  1. -1.]\n",
      " [ 0. -1.  0.]]\n",
      "Enter your value among :\n",
      "[2, 3, 5, 7]\n",
      "enter value8\n"
     ]
    }
   ],
   "source": [
    "game.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_list= []\n",
    "k = 0\n",
    "for key,val in enumerate(game.nwin_list):\n",
    "    k+=val\n",
    "    if(key%100==0):\n",
    "        win_list.append(k)\n",
    "        k=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 55, 26, 27, 48, 43, 45, 73, 85, 89, 84, 74, 79, 83, 91, 87, 68, 48, 25, 17]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x641faf950>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(win_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-faaab29a4947>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplot'"
     ]
    }
   ],
   "source": [
    "import matplot.plot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=9, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(9, activation='sigmoid'))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0,0,0,1,0,0,1,1,0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11872599, 0.10063873, 0.10116652, 0.12087925, 0.10957962,\n",
       "        0.11780806, 0.10732874, 0.11861013, 0.10526288]], dtype=float32)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[0,0,0,1,0,0,1,1,0]])\n",
    "p = model.predict(a)\n",
    "# allowed_action_prob = np.take(p,allowed)\n",
    "# np.argmax(allowed_action_prob)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,1,0,1,0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_position = a==1\n",
    "allowed_action = np.where(allowed_position == True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = Env(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0. -1. -1. -1. -1. -1. -1. -1. -1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((array([ 0., -1., -1., -1., -1., -1., -1., -1., -1.]),\n",
       "  [1, 2, 3, 4, 5, 6, 7, 8]),\n",
       " 0,\n",
       " False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent1.get_board()\n",
    "agent1.step(action=0,mark=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., -1., -1.],\n",
       "       [-1., -1., -1.],\n",
       "       [-1., -1., -1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent1.get_board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[ 0. -1. -1. -1. -1.  0. -1. -1. -1.]\n",
      "[-1.  1. -1. -1.  1. -1. -1. -1. -1.]\n",
      "[ 0. -1. -1. -1. -1.  0.  0. -1. -1.]\n",
      "[-1.  1. -1. -1.  1. -1. -1.  1. -1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((array([-1.,  1., -1., -1.,  1., -1., -1.,  1., -1.]), [0, 2, 3, 5, 6, 8]),\n",
       " 100,\n",
       " True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent2 = Env(1)\n",
    "# agent2.get_board()\n",
    "agent2.step(1)\n",
    "agent1.step(5)\n",
    "agent2.step(4)\n",
    "agent1.step(6)\n",
    "agent2.step(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  1., -1.],\n",
       "       [-1.,  1., -1.],\n",
       "       [-1.,  1., -1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent2.get_board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1. -1. -1. -1.  0.]\n",
      "(array([-1., -1., -1., -1., -1., -1., -1., -1.,  0.]), [0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "state, reward, done = agent1.step(8)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[-1. -1. -1. -1.  0. -1. -1. -1. -1.]\n",
      "agent1 made a move\n",
      "[[-1. -1. -1.]\n",
      " [-1.  0. -1.]\n",
      " [-1. -1. -1.]]\n",
      "[-1. -1. -1. -1.  0. -1. -1. -1.  1.]\n",
      "agent 2 made a move\n",
      "[[-1. -1. -1.]\n",
      " [-1.  0. -1.]\n",
      " [-1. -1.  1.]]\n",
      "True\n",
      "[-1. -1. -1. -1.  0.  0. -1. -1.  1.]\n",
      "agent1 made a move\n",
      "[[-1. -1. -1.]\n",
      " [-1.  0.  0.]\n",
      " [-1. -1.  1.]]\n",
      "[-1.  1. -1. -1.  0.  0. -1. -1.  1.]\n",
      "agent 2 made a move\n",
      "[[-1.  1. -1.]\n",
      " [-1.  0.  0.]\n",
      " [-1. -1.  1.]]\n",
      "True\n",
      "[-1.  1. -1. -1.  0.  0. -1.  0.  1.]\n",
      "agent1 made a move\n",
      "[[-1.  1. -1.]\n",
      " [-1.  0.  0.]\n",
      " [-1.  0.  1.]]\n",
      "[ 1.  1. -1. -1.  0.  0. -1.  0.  1.]\n",
      "agent 2 made a move\n",
      "[[ 1.  1. -1.]\n",
      " [-1.  0.  0.]\n",
      " [-1.  0.  1.]]\n",
      "True\n",
      "[ 1.  1.  0. -1.  0.  0. -1.  0.  1.]\n",
      "agent1 made a move\n",
      "[[ 1.  1.  0.]\n",
      " [-1.  0.  0.]\n",
      " [-1.  0.  1.]]\n",
      "[ 1.  1.  0. -1.  0.  0.  1.  0.  1.]\n",
      "agent 2 made a move\n",
      "[[ 1.  1.  0.]\n",
      " [-1.  0.  0.]\n",
      " [ 1.  0.  1.]]\n",
      "True\n",
      "[1. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      "agent1 won\n"
     ]
    }
   ],
   "source": [
    "agent1 = Game(0)\n",
    "agent2 = Game(1)\n",
    "Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "plot_every = num_episodes/25\n",
    "score_collection = []\n",
    "scores = []\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    episode_reward = 0\n",
    "    agent0.reset()\n",
    "    agent1.reset()\n",
    "    while True:\n",
    "        play = np.random.choice([True,False],1,p=[0.9,0.1])[0]\n",
    "        print(play)\n",
    "        if(play):\n",
    "            a1_action = np.random.choice(agent1.allowedAction)\n",
    "            state1, reward1, done = agent1.step(a1_action)\n",
    "            agent2.set_board(state1[0])\n",
    "            if(done):\n",
    "                if(reward1 == 100):\n",
    "                    print('agent1 won')\n",
    "                else:\n",
    "                    print('draw')\n",
    "                break\n",
    "            else:\n",
    "                print('agent1 made a move')\n",
    "                print(agent1.get_board())\n",
    "\n",
    "\n",
    "        play = np.random.choice([True,False],1,p=[0.9,0.1])[0]\n",
    "        if(play):\n",
    "            a2_action = np.random.choice(agent2.allowedAction)\n",
    "    #         a2_action = int(input(\"input a number among \"+ str(agent2.allowedAction)))\n",
    "            state2, reward2, done = agent2.step(a2_action)\n",
    "            agent1.set_board(state2[0])\n",
    "            if(done):\n",
    "                if(reward2 == 100):\n",
    "                    print('agent2 won')\n",
    "                else:\n",
    "                    print('draw')\n",
    "                break\n",
    "            else:\n",
    "                print('agent 2 made a move')\n",
    "                print(agent2.get_board())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def q_learning(env, num_episodes, alpha=0.1, gamma=1.0, epsilon=0.05):\n",
    "    # initialize action-value function (empty dictionary of arrays)\n",
    "    Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "    # initialize performance monitor -\n",
    "#     Get a0 for s0 based on epsilon\n",
    "    plot_every = num_episodes/25\n",
    "    # loop over episodes\n",
    "    score_collection = []\n",
    "    scores = []\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        episode_reward = 0\n",
    "#         How it acts in each episode\n",
    "        state = env.reset()\n",
    "        action = performAction(env,state,Q,epsilon)\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            steps+=1\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode_reward += reward\n",
    "#             list_state = s[1:-1].split(\",\")\n",
    "#             if(int(list_state[1])==10 and int(list_state[0])==10):\n",
    "#             print('\\r', 'state: ', state,'i: ', i, end='')\n",
    "            if not done:\n",
    "                next_action = performAction(env,next_state,Q,epsilon)\n",
    "                Q[state][action] = Q[state][action] + (alpha * (reward + (gamma * Q[next_state][next_action]) - Q[state][action]))\n",
    "                state = next_state\n",
    "            else:\n",
    "                Q[state][action] = Q[state][action] + (alpha * (reward + (gamma * Q[next_state][next_action]) - Q[state][action]))\n",
    "                score_collection.append(episode_reward)\n",
    "        \n",
    "        # monitor progress\n",
    "        if i_episode % 10 == 0:\n",
    "            scores.append(np.mean(score_collection))\n",
    "#             print(\"\\rEpisode {}/{}: Reward {}\".format(i_episode, num_episodes, episode_reward), end=\"\")\n",
    "#             sys.stdout.flush()\n",
    "            print('\\r', 'i_episode: ', i_episode, end='')\n",
    "            score_collection = []\n",
    "        ## TODO: complete the function\n",
    "    \n",
    "    # plot performance\n",
    "    plt.plot(np.linspace(0,num_episodes,len(scores),endpoint=False),np.asarray(scores))\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.ylabel('Average Reward (Over Next %d Episodes)' % plot_every)\n",
    "    plt.show()\n",
    "    # print best 100-episode performance\n",
    "    print(('Best Average Reward over %d Episodes: ' % plot_every), np.max(scores))\n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,1,1,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.where(a==2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play = np.random.choice([True,False],1,p=[0.9,0.1])[0]\n",
    "play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
