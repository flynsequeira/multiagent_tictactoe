{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience Replay\n",
    "class ReplayBuffer():\n",
    "    def __init__(self,memory_size=100000, input_shape=(9,)):\n",
    "        self.memory_size = int(memory_size/1.5)\n",
    "        self.input_shape = input_shape\n",
    "        # memory counter -> tracks first unsaved memory and use it to insert new memory into buffer\n",
    "        # When replay buffer becomes full, it rewrites earliest memoreis\n",
    "        self.memory_counter = 0\n",
    "#         print(input_shape)\n",
    "#         print(type(self.memory_size/2))\n",
    "        self.state_memory = np.zeros((self.memory_size, *input_shape), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.memory_size, *input_shape), dtype=np.float32)\n",
    "        # terminal memory is the memory where it ends. \n",
    "        # Try int32 \n",
    "        self.action_memory = np.zeros(self.memory_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.memory_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.memory_size, dtype=np.float32)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = 1-int(done)\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        # Checklist: Have we filled up the agents memory or not. If we have we need to sample a bunch of zeroes\n",
    "        # If we haven't filled up the agent's memory we shouldn't sample it\n",
    "        # If we filled it up -> we sample full memory \n",
    "        # If we haven't -> we sample till the memory size, because the rest doesn't give it any significant information\n",
    "\n",
    "        # TODO: Remove this option and test it out\n",
    "        current_memory = min(self.memory_counter, self.memory_size)\n",
    "        # TODO: Constant batch size\n",
    "        batch = np.random.choice(current_memory, batch_size, replace=False)\n",
    "\n",
    "        # THEN WE JUST SAMPLE THE MEMORY\n",
    "        state = self.state_memory[batch]\n",
    "        state_ = self.new_state_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        # TODO: Instead of the above random choice the s,a,r,s',t from the list directly of size, batch size, instead we can build a better np arrays\n",
    "        # Simpler? YES!\n",
    "\n",
    "        return state, actions, rewards, state_, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# To copy weights of local to target network\n",
    "import copy\n",
    "\n",
    "class Env:\n",
    "    def __init__(self):\n",
    "        self.board = np.array([-1.0] * 9)\n",
    "        self.allowedAction = np.where([-1.0] * 9)\n",
    "        self.winning_combos = (\n",
    "        [6, 7, 8], [3, 4, 5], [0, 1, 2], [0, 3, 6], [1, 4, 7], [2, 5, 8],\n",
    "        [0, 4, 8], [2, 4, 6],)\n",
    "        self.corners = [0,2,6,8]\n",
    "        self.sides = [1,3,5,7]\n",
    "        self.middle = 4\n",
    "\n",
    "    def get_marker(self):\n",
    "        return (1.0,0.0)\n",
    "    \n",
    "    def set_board(self, board):\n",
    "        self.board = board\n",
    "        self.allowedAction = list(np.where(self.board==-1)[0])\n",
    "        return board\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.array([-1.0] * 9)\n",
    "        return self.board\n",
    "    \n",
    "    def get_board(self):\n",
    "        return self.board.reshape(3,3)\n",
    "\n",
    "    def step(self, action, mark):\n",
    "        over = False\n",
    "        reward = 0\n",
    "\n",
    "        self.make_move(self.board, action, mark)\n",
    "        \n",
    "        if(self.is_winner(self.board,mark)):\n",
    "            reward = 100\n",
    "            over = True\n",
    "\n",
    "        # drawing\n",
    "        elif self.is_board_full():\n",
    "            reward = 10\n",
    "            over = True\n",
    "        \n",
    "#         print(self.board)\n",
    "\n",
    "        self.allowedAction = list(np.where(self.board==-1)[0])\n",
    "\n",
    "        return self.board,self.allowedAction, reward, over\n",
    "\n",
    "    def is_winner(self, board, mark):\n",
    "        for combo in self.winning_combos:\n",
    "            if (board[combo[0]] == board[combo[1]] == board[combo[2]] == mark):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_winning_combo(self, board):\n",
    "        for combo in self.winning_combos:\n",
    "            if (board[combo[0]] == board[combo[1]] == board[combo[2]]):\n",
    "                return [combo[0], combo[1], combo[2]]\n",
    "        return [None, None, None]\n",
    "\n",
    "    def is_space_free(self, board, index):\n",
    "        \"checks for free space of the board\"\n",
    "        return board[index] == -1.0\n",
    "\n",
    "    def is_board_full(self):\n",
    "        \"checks if the board is full\"\n",
    "        for i in range(1,9):\n",
    "            if self.is_space_free(self.board, i):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def make_move(self,board,index, mark):\n",
    "        board[index] =  mark\n",
    "\n",
    "    def choose_random_move(self, move_list):\n",
    "        possible_winning_moves = []\n",
    "        for index in move_list:\n",
    "            if self.is_space_free(self.board, index):\n",
    "                possible_winning_moves.append(index)\n",
    "        if len(possible_winning_moves) != 0:\n",
    "            return random.choice(possible_winning_moves)\n",
    "        else:\n",
    "            return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self, epsilon=1., min_epsilon=0.1, epsilon_decay=5e-4):\n",
    "        self.epsilon = 1\n",
    "        self.batch_size = 50\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.game = Env()\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(4, input_dim=9, activation='relu'))\n",
    "        self.model.add(Dense(4, activation='relu'))\n",
    "        self.model.add(Dense(9, activation='sigmoid'))\n",
    "        self.model.add(Activation('softmax'))\n",
    "        self.model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "        self.model_adv = Sequential()\n",
    "        self.model_adv.add(Dense(4, input_dim=9, activation='relu'))\n",
    "        self.model_adv.add(Dense(4, activation='relu'))\n",
    "        self.model_adv.add(Dense(9, activation='sigmoid'))\n",
    "        self.model_adv.add(Activation('softmax'))\n",
    "        self.model_adv.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "        self.buffer = ReplayBuffer(input_shape = (9,) , memory_size = 100000)\n",
    "        self.buffer_adv = ReplayBuffer(input_shape = (9,) , memory_size = 100000)\n",
    "#         self.model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    def take_action(self, allowed_actions, state):\n",
    "#         FOR SELECTING RANDOM\n",
    "        random = np.random.choice([True, False], 1, p=[self.epsilon,1-self.epsilon])[0]\n",
    "#     UPDATE EPSILON VALUE\n",
    "        self.epsilon = max(self.epsilon-self.epsilon_decay, self.min_epsilon)\n",
    "        \n",
    "#         IF RANDOM WHAT DO I DO!!!!\n",
    "        all_actions = np.arange(9)\n",
    "\n",
    "#         IF RANDOM WE GO WITH THIS TWO LINES\n",
    "        allowed_actions_pos = np.take(all_actions, allowed_actions)\n",
    "        action = np.random.choice(allowed_actions_pos)\n",
    "            \n",
    "        if not random:\n",
    "            actions = self.model.predict(np.array([state]))[0]\n",
    "            max_action = 0\n",
    "            for k,v in enumerate(list(actions)):\n",
    "                if k in allowed_actions:\n",
    "                    if v>max_action:\n",
    "                        max_action=v\n",
    "                        action = k\n",
    "        return action\n",
    "    \n",
    "    def play_with_self(self):\n",
    "\n",
    "        for i in range(10000):\n",
    "            self.game.reset()\n",
    "            done = False\n",
    "            if i%1000 == 0:\n",
    "                print(\"rounds: \", i)\n",
    "            step = 0\n",
    "            allowed_actions = np.arange(9)\n",
    "            state = self.game.board\n",
    "            while not done:\n",
    "#                 Starting game\n",
    "                step+=1\n",
    "#                 When player 1 is done, player 2 get's lower reward\n",
    "\n",
    "                p1_play = np.random.choice([True,False], p = [0.9,0.1])\n",
    "                if(p1_play):\n",
    "#                 Player 1\n",
    "                    action = self.take_action(allowed_actions, state)\n",
    "                    state_,allowed_actions,r,done = self.game.step(action=action,mark=0)\n",
    "                \n",
    "                p2_play = np.random.choice([True,False], p = [0.9,0.1])\n",
    "#                 Player 2\n",
    "                if p2_play:\n",
    "                    if not done:\n",
    "                        action2 = self.take_action(allowed_actions, state_)\n",
    "                        state2_,allowed_actions,r2,done = self.game.step(action=action2,mark=1)\n",
    "                        self.buffer_adv.store_transition(state_, action2, r2, state2_, done)\n",
    "                    else:\n",
    "                        self.buffer_adv.store_transition(state, action, -r, state_, done)\n",
    "                \n",
    "                if(p1_play):\n",
    "    #                 Add both to ReplayBuffer\n",
    "                    self.buffer.store_transition(state, action, r, state_, done)\n",
    "                \n",
    "                self.learn()\n",
    "                state = state2_\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.buffer.memory_counter<self.batch_size:\n",
    "            return\n",
    "        \n",
    "        state, action, rewards, state_, dones = self.buffer.sample_buffer(self.batch_size)\n",
    "        state_adv, action_adv, rewards_adv, state_adv_, dones = self.buffer_adv.sample_buffer(self.batch_size)\n",
    "\n",
    "        # Finding a q value for a bunch of states and next states\n",
    "        q = self.model.predict(state)\n",
    "        q_next = self.model.predict(state_)\n",
    "\n",
    "        # Finding a q value for a bunch of states and next states\n",
    "        q_adv = self.model.predict(state_adv)\n",
    "        q_adv_next = self.model.predict(state_adv_)\n",
    "\n",
    "        # Q value to be updated -> basically the weights to be updated such that the q value is changed to whatever it is supposed to be. \n",
    "        q_target = np.copy(q)\n",
    "        q_target_adv = np.copy(q)\n",
    "\n",
    "        # handles array indexing later\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "#       MaxQ\n",
    "        max_of_q_of_each_sample = np.max(q_next, axis=1)\n",
    "        max_of_q_of_each_sample_adv = np.max(q_adv_next, axis=1)\n",
    "#     Gamma = 0.9\n",
    "        q_target[batch_index, action] = rewards+0.9*max_of_q_of_each_sample*dones\n",
    "        q_target_adv[batch_index, action] = rewards+0.9*max_of_q_of_each_sample_adv*dones\n",
    "\n",
    "#       IS THERE ANY OTHER WAY OF WRITING THIS?\n",
    "#       Neural Network style of updating a q table. Update weights\n",
    "        self.model.train_on_batch(state, q_target)\n",
    "        self.model_adv.train_on_batch(state_adv, q_target_adv)\n",
    "\n",
    "        # TODO: Update self.epsilon\n",
    "\n",
    "        self.epsilon = self.epsilon - self.epsilon_decay if self.epsilon>self.min_epsilon else self.min_epsilon\n",
    "\n",
    "    def play(self):\n",
    "        done = False\n",
    "        step = 0\n",
    "        allowed_actions = np.arange(9)\n",
    "        self.game.reset()\n",
    "        state = self.game.board\n",
    "        while not done:\n",
    "#                 Starting game\n",
    "            step+=1\n",
    "            p1_play = np.random.choice([True,False], p = [0.9,0.1])\n",
    "            if(p1_play):\n",
    "    #                 Player 1\n",
    "                action = self.take_action(allowed_actions, state)\n",
    "                state_,allowed_actions,r,done = self.game.step(action=action,mark=0)\n",
    "            if(p2_play):\n",
    "    #                 Player 2\n",
    "                print('Game: ')\n",
    "                print(self.game.get_board())\n",
    "                print(\"Enter your value among :\") \n",
    "                print(self.game.allowedAction)\n",
    "                played_right = False\n",
    "                while not played_right:\n",
    "                    try:\n",
    "                        action2 = int(input('enter value'))\n",
    "                        played_right = True\n",
    "                    except:\n",
    "                        print(\"Enter the right value in integer from 0 to 8\")\n",
    "                state2_,allowed_actions,r2,done = self.game.step(action=action2,mark=1)\n",
    "            \n",
    "            state = state_\n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rounds:  0\n",
      "rounds:  1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ce9f32c88f9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_with_self\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-9cb71d548fc1>\u001b[0m in \u001b[0;36mplay_with_self\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m#                 Player 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallowed_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstate_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallowed_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmark\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-9cb71d548fc1>\u001b[0m in \u001b[0;36mtake_action\u001b[0;34m(self, allowed_actions, state)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mmax_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mindices_for_conversion_to_dense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.7/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    321\u001b[0m                     \u001b[0;34m'forgot to call `super(YourClass, self).__init__()`.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                     ' Always start with this line.')\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[0m_DISABLE_TRACKING\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_DISABLE_TRACKING\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1218\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1219\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_metrics'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "game = Game()\n",
    "game.play_with_self()\n",
    "game.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([True,False], p = [0.9,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
